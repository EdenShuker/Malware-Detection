# Malware Detection
This project's main goal is to classify benign files and variants of malware files into their respective families. It consist of 2 parts, each one has a different approach to achieve this goal: one with XGBoost model and the second with deep learning model.

XGBoost Model:
Machine learning model using xgboost - a scalable and accurate implementation of gradient boosting machines.

Second model - Deep Learning:
Classification model based on convolutional network taken from 'Malware Detection by Eating a Whole EXE' paper written by Edward Raf, Jon Barker, Jared Sylvester, Robert Brandon, Bryan Catanzaro and Charles Nicholas (link: https://arxiv.org/pdf/1710.09435.pdf).


## Results:

### XGBoost Model:

XGBoost model was used over one class of benign files and 3 classes of malware taken from the Kaggle contest of 2015. Each file is Windows8 PE without the PE header. Those are the results:

|          |   Train  |   Test   |
|----------|----------|----------|
| Accuracy |   99.98  |   99.62  |

<img src="http://docs.google.com/uc?export=open&id=1vqAbXJv64k07RBZatj4OtfxbnmOe-FYf" width="769" height="527">

### Deep Learning Model:

Deep learning model was used one time as multiclass classifier for 3 classes and second time as binary classifier.

|  Accuracy  |  Train  |   Test  |
|------------|---------|---------|
|   Binary   |  99.95  |  97.18  |
| Multiclass |  95.32  |  90.11  |

![alt text](http://docs.google.com/uc?export=open&id=1du29cO38sOwU6Nxx2VZlM1cbaszHhuFU)

<img src="http://docs.google.com/uc?export=open&id=1dQ4WzU-IcKRMcvMYRxlshyneEKN2ZTHc">

## Requirements:
* XGBoost
* PyTorch
* PEFile
* Capstone
* Numpy

## Install:

TODO


## Running Instructions:

### Deep Learning Model:

This part uses the 'run.py' script in the directory 'deep_code'.

Run by:

      python3 run.py [options]

Options are:
* [-train configuration_file]
* [-save model_filename]
* [-load model_filename]
* [-eval configuration_file]
* [-test configuration_file]

#### Arguments:

NOTE - explanations on the '.yaml' files (configuration files) are after the parameters description.

1. Add '-train configuration_file' in order to train a model.
   
   configuration_file is a path to a '.yaml' file containing the configurations of the training.

2. Add '-save model_filename' in order to save the model after training.
   
   model_filename is a path to the file that will be created, will contain the saved model.

3. Add '-load model_filename' in order to load a saved model.
   
   model_filename is the same as in the save option.

4. Add '-eval configuration_file' in order to evaluate your model on a dataset.
   
   configuration_file is a path to a '.yaml' file containing the configurations of the validation.

5. Add '-test configuration_file' in order to do a blind prediction on files.
   
   configuration_file is a path to a '.yaml' file containing the configurations of the testing.

#### Configuration Files:

Each line in '.yaml' file is a key-value pair in the format - 'key: value' .

**Keys:**

* main_dir - (string) like 'some/path', path to the main directory, which contains sub-directories,
                      each sub-directory contains files of the respective family.

* first_n_byte - (int) number of bytes to read from each file in order to classify it.

* lr - (float) learning rate.

* num_epochs - (int) number of epochs for training.

* labels - (string) path to a file, each line in it is a label name.

* labels2dir - (string) path to '.csv' file, each line in it is in the format of 'label,dir'.
               maps a labels name to the name of the sub-directory in the main-directory.

* batch - (int) batch size to use in the train and dev data-loaders.

* workers - (int) number of workers to use in the train and dev data-loaders.

* conf_mat - (boolean) like True or False,
              set True for showing the confusion matrix in the last evaluation on dev.

* files_ls_path - (string) path to a file where each line in it is a path to a file to predict on.

* target_file - (string) path to a file that will be created after the prediction,
                line[i] in it is the label of files[i] in the file that was passed in FILES_LS_PATH.

The keys needed for each '.yaml' file:

| Train         | Eval          | Test          |
|---------------|---------------|---------------|
| main_dir      | main_dir      | files_ls_path |
| first_n_bytes | labels        | labels2dir    |
| lr            | labels2dir    | workers       |
| num_epochs    | batch         | first_n_bytes |
| labels        | workers       | target_file   |
| labels2dir    | first_n_bytes |               |
| batch         |               |               |
| workers       |               |               |
| conf_mat      |               |               |

### XGBoost Model:


# TODO change below


There are 2 shell scripts in project directory:

    1. run_ml_model.sh - runs xgboost model
    2. run_deep_model.sh - runs deep learning model

In order to run each model you only need to suit the parameters of the .py files in each shell script.
We'll describe here exactly how to do it and what is every parameter:

1. run_ml_model.sh

        split_data.py  [train_labals.csv]  [target_dir_path]  [split_ratio]

                train_labels.csv - csv file contains mapping from file path to its label for each file in train set.

                target_dir_path - dir to put in the train_set.csv and test_set.csv created from split. both csv files mapping path to label

                split_ratio - ratio size to give test set (validation set).
                
              
        ml_code/extract_ngrams.py -n [$i] -p [path/train_set.csv]
        
                $i - number of class
                
                path/train_set.csv - path to train_set.csv created before.
                
                This script goes over all files of specific class and extract 10,000 most common ngrams.
         
         
        ml_code/join_ngrams.py
            
                Pick 750 ngrams for each class by using cross-entropy and creates ngrams array = 750 * # classes


        ml_code/extract_segments.py
        
                For each file in train_set.csv - extract a list of segment names
                
                
        ml_code/f2v.py [path/train_set.csv]  [path_to_train.f2v_file]
        
                path/train_set.csv - path to train_set.csv created before.
                
                path_to_train.f2v_file - path to f2v file to create
                
                Go over all files in train set and represents them as vectors of features. Output file called train.f2v which maps every file to its features vec.
                

        ml_code/f2v.py [path/test_set.csv]  [path_to_test.f2v_file]
        
                path/test_set.csv - path to test_set.csv created before.
                
                path_to_test.f2v_file - path to f2v file to create
                
                Output file called test.f2v which maps every file in test set to its features vec.



2. run_deep_model.sh
    
        split_data.py  [train_labals.csv]  [target_dir_path]  [split_ratio]  --- same as before
                

        deep_code/model.py [...]  [..]  [..]


